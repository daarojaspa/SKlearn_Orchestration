{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a593f88",
   "metadata": {},
   "source": [
    "For me bagging is like a  random forest kind of dynamic where you put models to work in the same problem but in parallel. while boosting is like  putting  resistances on series, to get a bigger one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97655610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler  # Fixed: 'StandarScaler' â†’ 'StandardScaler'\n",
    "\n",
    "path=\"/home/dan/PLATZI/data/SKlearn_Orchestration/skLearn_Orchestration/data/third_part/\"\n",
    "file=\"heart.csv\"\n",
    "df=pd.read_csv(path+file)\n",
    "target=df[\"target\"]\n",
    "features=df.drop([\"target\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c282d5e",
   "metadata": {},
   "source": [
    "KNN Clasifier: you  mesure the distance from your new sample to the datapoints you already have and label it as the nears neghbors are label, need to define K and  the messure distance, is very adaptable, but not scalabel, so as the data set incrices   it gets worse, is normally used for imputation of missing values.\n",
    "\n",
    "is like memorising  a text to give the closess answer, bigger the text the harder it gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52561d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score for knn:0.7120622568093385\n",
      "================================================================\n",
      "accuracy score for bagging:0.7431906614785992\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(features,target,test_size=0.25,random_state=37)\n",
    "features=StandardScaler().fit_transform(features)\n",
    "knn=KNeighborsClassifier().fit(X_train,Y_train)\n",
    "knn_pred=knn.predict(X_test)\n",
    "print(f\"accuracy_score for knn:{accuracy_score(Y_test,knn_pred)}\")\n",
    "print(\"=\"*64)\n",
    "bagging=BaggingClassifier(estimator=KNeighborsClassifier(),n_estimators=10).fit(X_train,Y_train)\n",
    "bagging_pred=bagging.predict(X_test)\n",
    "print(f\"accuracy score for bagging:{accuracy_score(Y_test,bagging_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60666040",
   "metadata": {},
   "source": [
    "Gradiante Boosting:\n",
    "    - builds models secuentialy were each model corrects the errors of the previus ones\n",
    "    -it aplies gradient decent logic in the models\n",
    "    - the gradient desent optimice models ( it runs on the space of functions) not parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df9eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "accuracy score  for  boosting0.9610894941634242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "boost=GradientBoostingClassifier().fit(X_train,Y_train)\n",
    "boost_pred=boost.predict(X_test)\n",
    "print(\"=\"*64)\n",
    "print(f\"accuracy score  for  boosting{accuracy_score(Y_test,boost_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearnProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
